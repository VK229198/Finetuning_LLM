# Hands on FineTuning LLMs
## Description
Trying hands on with LLMs to put the theory to work and also recapping on PyTorch basics and HuggingFace systems.
A part of Learning
## Outcomes
The results are nothing to write home about. The base model was not that far off from finetuned model, however, learning to implement:
- finetuning basics
- a/b testing
- pytorch for LLMs
- huggingface models for tokenizing and using the base models
- learnt the effects of free open source LLMs and how to use them
- In fact saw in real time how finetuning helps us evolve a LLM to better suit a particular task
## Conclusion
This project was very rewarding for me as it helped me learn a lot of basics and made me sharper. It also helped me view LLMs from a different perspective, and while trying to implement this project, I learnt a lot of new concepts related to this topic. This helps me a lot especially for a future project for my courses, NLP and Gen AI, and hopefully implement my own SLM/LLM. 
- Learnt crucial fundamentals
- Learnt a lot of easy to miss without a deep dive into the subject topics, like RLHF and types of fintuning based on tasks
- Learnt fundamentals of building an LLM from scratch and the basics of Attention mechanisms( self, casual and multi-head) and even learnt about transformers.
- The goal of the project was to learn a lot on this topic and I think it is safe to say, it has been acomplished

### A huge credit goes to the book "Build A Large Language Model From Scratch" by Sebastian Raschka as i picked up so many fundamentals which were previously missed
